## ğŸ“˜ DÃA 5 â€” Agregaciones y funciones de ventana en Spark

### ğŸ¯ Objetivo general

Comprender cÃ³mo Spark ejecuta **agregaciones por grupo (`groupBy`)** y **funciones de ventana (`window functions`)**, cuÃ¡ndo usar cada una, y cÃ³mo aplicarlas para resolver problemas tÃ­picos de anÃ¡lisis y procesamiento secuencial de datos. Estos conceptos son esenciales tanto en entornos reales (pipelines ETL, KPIs, logs de usuariosâ€¦) como en **entrevistas tÃ©cnicas**.

---

## ğŸ§  1. Agregaciones con `groupBy` y `agg`

El mÃ©todo `groupBy` permite **agrupar filas** que comparten un mismo valor en una o varias columnas y aplicar funciones agregadas como `sum`, `count`, `avg`, `max`, `min`, etc.

**Ejemplo bÃ¡sico:**

```scala
val df = Seq(
  ("LucÃ­a", "ES", 100),
  ("LucÃ­a", "ES", 200),
  ("Miguel", "PT", 80)
).toDF("name", "country", "amount")

val grouped = df.groupBy("country").agg(
  sum("amount").alias("total_sales"),
  count("*").alias("num_records")
)

grouped.show()
```

**Salida (tabla conceptual):**
country | total_sales | num_records
ES | 300 | 2
PT | 80 | 1

**Idea clave:** `groupBy` **reduce filas**: si habÃ­a 10.000 filas y agrupas por â€œclienteâ€, el resultado tendrÃ¡ 1 fila por cliente.

---

## âš™ï¸ 2. Agregaciones mÃºltiples y expresivas

Puedes aplicar varias funciones de agregaciÃ³n en una sola operaciÃ³n:

```scala
df.groupBy("country").agg(
  round(avg("amount"), 2).alias("avg_sales"),
  max("amount").alias("max_sale"),
  min("amount").alias("min_sale")
)
```

Agregaciones **condicionales** con `when` dentro de `agg`:

```scala
df.groupBy("country").agg(
  sum(when(col("amount") > 100, 1).otherwise(0)).alias("high_value_sales")
)
```

---

## ğŸ§± 3. Funciones de ventana (window functions)

A diferencia de `groupBy`, las **funciones de ventana no reducen el nÃºmero de filas**; aÃ±aden **columnas calculadas** basadas en otras filas â€œrelacionadasâ€ (por grupo, por orden, etc.).

**Imports necesarios:**

```scala
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._
```

---

## ğŸ“ 4. Estructura de una ventana

```scala
val w = Window.partitionBy("columna_grupo").orderBy("columna_orden")
```

* **partitionBy**: define **el grupo** (cliente, paÃ­s, etc.)
* **orderBy**: define **el orden** dentro del grupo (fecha, importe, etc.)
* Aplicas funciones con `.over(w)`.

---

## ğŸ” 5. Ejemplo bÃ¡sico de ventana

```scala
val sales = Seq(
  ("LucÃ­a", "2023-01-01", 100),
  ("LucÃ­a", "2023-01-05", 50),
  ("LucÃ­a", "2023-01-10", 120),
  ("Miguel", "2023-01-02", 80),
  ("Miguel", "2023-01-09", 150)
).toDF("customer", "date", "amount")

val w = Window.partitionBy("customer").orderBy("date")

val withRunningTotal = sales.withColumn("running_total", sum("amount").over(w))
```

**Resultado (tabla conceptual):**
customer | date | amount | running_total
LucÃ­a | 2023-01-01 | 100 | 100
LucÃ­a | 2023-01-05 | 50  | 150
LucÃ­a | 2023-01-10 | 120 | 270
Miguel | 2023-01-02 | 80  | 80
Miguel | 2023-01-09 | 150 | 230

**Idea clave:** el nÃºmero de filas **no cambia**, pero cada fila â€œsabe lo que pasÃ³ antesâ€.

---

## ğŸ”¢ 6. Funciones comunes de ventana

| FunciÃ³n               | DescripciÃ³n                       | Ejemplo                     |
| --------------------- | --------------------------------- | --------------------------- |
| `row_number()`        | Numera las filas dentro del grupo | `row_number().over(w)`      |
| `rank()`              | Ranking con huecos por empates    | `rank().over(w)`            |
| `dense_rank()`        | Ranking sin huecos                | `dense_rank().over(w)`      |
| `lag(col, n)`         | Valor anterior de `col`           | `lag("amount", 1).over(w)`  |
| `lead(col, n)`        | Valor siguiente de `col`          | `lead("amount", 1).over(w)` |
| `sum()/avg()/count()` | Acumulados por ventana            | `sum("amount").over(w)`     |

---

## ğŸ§© 7. ComparaciÃ³n `groupBy` vs `window`

| Aspecto           | `groupBy`             | `window`                          |
| ----------------- | --------------------- | --------------------------------- |
| Agrupa datos      | âœ… SÃ­                  | ğŸš« No                             |
| Reduce filas      | âœ… SÃ­                  | ğŸš« No                             |
| Mantiene columnas | ğŸš« No                 | âœ… SÃ­                              |
| Se usa para       | Totales y resÃºmenes   | Rankings, acumulados, diferencias |
| Ejemplo           | Total ventas por paÃ­s | Ventas acumuladas por cliente     |

---

## ğŸ’¬ 8. Ejemplo con `lag`, `lead` y `row_number`

```scala
val w = Window.partitionBy("customer").orderBy("date")

val windowed = sales
  .withColumn("purchase_num", row_number().over(w))
  .withColumn("diff_vs_prev", col("amount") - lag("amount", 1).over(w))
  .withColumn("next_amount", lead("amount", 1).over(w))
  .withColumn("avg_until_now", round(avg("amount").over(w), 2))

windowed.show()
```

**Resultado (tabla conceptual):**

| customer | date       | amount | purchase_num | diff_vs_prev | next_amount | avg_until_now |
| -------- | ---------- | ------ | ------------ | ------------ | ----------- | ------------- |
| LucÃ­a    | 2023-01-01 | 100    | 1            | null         | 50          | 100.0         |
| LucÃ­a    | 2023-01-05 | 50     | 2            | -50          | 120         | 75.0          |
| LucÃ­a    | 2023-01-10 | 120    | 3            | 70           | null        | 90.0          |
| Miguel   | 2023-01-02 | 80     | 1            | null         | 150         | 80.0          |
| Miguel   | 2023-01-09 | 150    | 2            | 70           | null        | 115.0         |

---

## ğŸ§  9. Conclusiones teÃ³ricas

* **`groupBy`**: reduce datos; ideal para totales, sumas, medias, KPIs.
* **`window`**: mantiene todas las filas; ideal para acumulados, rankings, diferencias.
* Las funciones ventana son **muy potentes** en Spark SQL.
* Combinadas con `when`, `lag`, `lead` y `rank`, generan transformaciones analÃ­ticas expresivas.
* En entrevistas es frecuente: â€œÃºltima fila por usuarioâ€, â€œtop N por grupoâ€, â€œrunning total por clienteâ€.

---

# ğŸ§© RETO DEL DÃA 5 â€” Ranking y acumulado de ventas

## ğŸ“ Dataset: `sales_data.json`

Guarda en `src/main/resources/sales_data.json`:

```json
[
  {"region": "South", "salesperson": "LucÃ­a", "date": "2023-01-01", "amount": 500},
  {"region": "South", "salesperson": "LucÃ­a", "date": "2023-01-05", "amount": 200},
  {"region": "South", "salesperson": "LucÃ­a", "date": "2023-01-10", "amount": 800},
  {"region": "North", "salesperson": "Miguel", "date": "2023-01-02", "amount": 300},
  {"region": "North", "salesperson": "Miguel", "date": "2023-01-09", "amount": 400},
  {"region": "North", "salesperson": "Carla", "date": "2023-01-03", "amount": 1000},
  {"region": "North", "salesperson": "Carla", "date": "2023-01-04", "amount": 200}
]
```

### ğŸ¯ Objetivos

1. Leer el JSON `sales_data.json`.
2. Por **vendedor (`salesperson`)** calcular:

    * **NÃºmero de venta** (`row_number`)
    * **Acumulado de ventas** (`sum.over(window)`)
    * **Media acumulada** (`avg.over(window)`)
    * **Diferencia con la venta anterior** (`lag`)
3. Calcular el **total de ventas por regiÃ³n** (`groupBy` + `agg`).
4. **Unir** ambos resultados en un Ãºnico DataFrame (`join`).
5. **Escribir** el resultado final en CSV en `src/main/resources/day05_output`.

### ğŸ’¬ Extra (nivel entrevista)

* Ranking global de vendedores por ventas totales (`rank().over(Window.orderBy(desc("total_sales")))`).
* Top 2 vendedores por regiÃ³n.
