# ğŸ“˜ DÃ­a 4 â€“ Columnas complejas, estructuras anidadas y funciones avanzadas

## ğŸ¯ Objetivo
Aprender a trabajar con columnas de tipo `struct`, `array` y `map` en Spark, accediendo a campos internos y transformÃ¡ndolos en un esquema tabular mediante funciones avanzadas.

---

## ğŸ§  Conceptos clave

### ğŸ”¹ StructType
Una estructura que contiene varios campos, como un objeto JSON.  
Acceso con `col("campo.subcampo")` o `$"campo.subcampo"`.

### ğŸ”¹ ArrayType
Columna que contiene listas.  
Manipulaciones comunes:
- `explode` â†’ convierte cada elemento del array en una fila
- `size` â†’ nÃºmero de elementos
- `array_contains` â†’ comprueba si contiene un valor

### ğŸ”¹ MapType
Columna que contiene pares clave/valor.  
Acceso: `col("mapa")("clave")`.

### ğŸ”¹ Funciones Ãºtiles
- `withColumn` â†’ crear o reemplazar columnas
- `explode` â†’ expandir arrays
- `when` / `otherwise` â†’ condiciones
- `lower`, `trim`, `size`, `struct`, `concat_ws` â†’ transformaciones comunes

---

## ğŸ’¡ Reto prÃ¡ctico

Dataset: `user_events.json`  
Cada fila representa un usuario con perfil, preferencias, eventos y metadatos.

Tareas:
1. Leer el JSON con estructuras anidadas
2. Extraer campos internos (`profile.name`, `metadata.ip`, etc.)
3. Derivar columnas nuevas (`country_lower`, `num_preferences`, `is_mobile`)
4. Hacer `explode` sobre los eventos
5. Clasificar usuarios con `when` / `otherwise`
6. Guardar el resultado final en CSV

---

## ğŸ“ Estructura de salida

src/main/resources/day04_output/
part-00000.csv

yaml
Copiar cÃ³digo

---

## ğŸ§© Conclusiones

- Spark permite manejar datos anidados directamente desde JSONs complejos.
- `explode` y `struct` son esenciales para â€œnormalizarâ€ informaciÃ³n jerÃ¡rquica.
- La evaluaciÃ³n perezosa sigue aplicando: nada se ejecuta hasta que hay una acciÃ³n (`write`, `show`, etc.).
- Este tipo de transformaciones aparecen constantemente en pipelines reales de streaming, logs o datos de APIs.
